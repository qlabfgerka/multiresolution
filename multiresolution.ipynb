{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torchinfo\n",
    "import skimage\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "TRAIN_FOLDER = 'train/'\n",
    "TEST_FOLDER = 'test/'\n",
    "\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "WINDOW_SIZE = 33\n",
    "WINDOW_OFFSET = 14\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutliresolution(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.first_conv2d = torch.nn.Conv2d(1, 64, kernel_size=(9, 9), padding=\"same\")\n",
    "        self.second_conv2d = torch.nn.Conv2d(64, 32, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.third_conv2d = torch.nn.Conv2d(32, 1, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.second_conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.third_conv2d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_image(image):\n",
    "    image = image / 255\n",
    "    image[image > 1] = 1\n",
    "    image[image < 0] = 0\n",
    "    image = image.astype(np.float32)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(image):\n",
    "    image = image.transpose(-1, 0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        image = cv2.imread(os.path.join(folder, filename), -1)\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            images.append(fix_image(image))\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, cmap=\"viridis\"):\n",
    "  plt.imshow(image, cmap=cmap)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_org_lr_hr(img_org, img_lr, img_hr, metrics, zoom_pos, zoom_size=(50, 50)):\n",
    "  _, axes = plt.subplots(2, 3, figsize=(10, 5))\n",
    "  \n",
    "  img_hr = cv2.cvtColor(img_hr.transpose(1, 2, 0), cv2.COLOR_YCR_CB2BGR)\n",
    "  img_hr[img_hr < 0] = 0\n",
    "  img_hr[img_hr > 1] = 1\n",
    "\n",
    "  axes[0, 0].imshow(img_org)\n",
    "  axes[0, 0].add_patch(patches.Rectangle(zoom_pos, *zoom_size, edgecolor=\"r\", facecolor=\"none\"))\n",
    "  axes[0, 0].axis(\"off\")\n",
    "  axes[0, 0].set_title(\"ORG / PSNR / SSIM\")\n",
    "\n",
    "  axes[0, 1].imshow(img_lr)\n",
    "  axes[0, 1].add_patch(patches.Rectangle(zoom_pos, *zoom_size, edgecolor=\"r\", facecolor=\"none\"))\n",
    "  axes[0, 1].axis(\"off\")\n",
    "  axes[0, 1].set_title(f\"LR / {metrics['psnr_org_lr'] :.2f} dB / {metrics['ssim_org_lr'] :.4f}\")\n",
    "\n",
    "  axes[0, 2].imshow(img_hr)\n",
    "  axes[0, 2].add_patch(patches.Rectangle(zoom_pos, *zoom_size, edgecolor=\"r\", facecolor=\"none\"))\n",
    "  axes[0, 2].axis(\"off\")\n",
    "  axes[0, 2].set_title(f\"HR / {metrics['psnr_org_hr'] :.2f} dB /␣{metrics['ssim_org_hr'] :.4f}\")\n",
    "\n",
    "  axes[1, 0].imshow(img_org[zoom_pos[1]:zoom_pos[1]+zoom_size[1], zoom_pos[0]:zoom_pos[0]+zoom_size[0]])\n",
    "  axes[1, 0].axis(\"off\")\n",
    "\n",
    "  axes[1, 1].imshow(img_lr[zoom_pos[1]:zoom_pos[1]+zoom_size[1], zoom_pos[0]:zoom_pos[0]+zoom_size[0]])\n",
    "  axes[1, 1].axis(\"off\")\n",
    "\n",
    "  axes[1, 2].imshow(img_hr[zoom_pos[1]:zoom_pos[1]+zoom_size[1], zoom_pos[0]:zoom_pos[0]+zoom_size[0]])\n",
    "  axes[1, 2].axis(\"off\")\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_filters(filters, title):\n",
    "    # [N, C, H, W] -> [N, H, W, C]\n",
    "    filters = filters.permute(0, 2, 3, 1)\n",
    "    filters = filters.clamp(0, 1)\n",
    "\n",
    "    # Sortiraj po padajoči varianci\n",
    "    indices = torch.argsort(torch.var(filters, axis=(1, 2, 3)), descending=True)\n",
    "    filters = filters[indices]\n",
    "    _, axes = plt.subplots(4, 16, figsize=(16, 4))\n",
    "\n",
    "    for r in range(axes.shape[0]):\n",
    "        for c in range(axes.shape[1]):\n",
    "            if r == 0 and c == 0:\n",
    "                axes[r, c].set_title(title)\n",
    "            axes[r, c].imshow(filters[r * axes.shape[1] + c], cmap=\"gray\")\n",
    "            axes[r, c].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, num, f1, f2, title=\"\"):\n",
    "    indices = np.random.default_rng().choice(len(images), num, replace=False)\n",
    "    _, axes = plt.subplots(f1, f2, figsize=(8, 3), squeeze=False)\n",
    "\n",
    "    for r in range(axes.shape[0]):\n",
    "        for c in range(axes.shape[1]):\n",
    "            if r == 0 and c == 0:\n",
    "                axes[r, c].set_title(title)\n",
    "            axes[r, c].imshow(images[indices[r * axes.shape[1] + c]])\n",
    "            axes[r, c].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info(images, title, num, f1, f2):\n",
    "    heights = list(map(lambda image: image.shape[0], images))\n",
    "    widths = list(map(lambda image: image.shape[1], images))\n",
    "\n",
    "    print(f\"Število slik: {len(images)}\")\n",
    "    print(f\"Interval širine slik: [{np.min(widths)}, {np.max(widths)}]\")\n",
    "    print(f\"Interval višine slik: [{np.min(heights)}, {np.max(heights)}]\")\n",
    "    print(f\"Povprečna širina slik: {np.mean(widths) :.2f} +- {np.std(widths) :.2f}\")\n",
    "    print(f\"Povprečna višina slik: {np.mean(heights) :.2f} +- {np.std(heights) :.2f}\")\n",
    "    print(\"Primeri slik:\")\n",
    "    display_images(images, num, f1, f2, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images, factor, type):\n",
    "    low_res = []\n",
    "    originals = []\n",
    "\n",
    "    for image in images:\n",
    "        smaller_size = (int(image.shape[1] / factor), int(image.shape[0] / factor))\n",
    "        bigger_size = (int(image.shape[1]), int(image.shape[0]))\n",
    "        \n",
    "        low_res_image = cv2.resize(image, smaller_size, cv2.INTER_LINEAR)\n",
    "        low_res_image = cv2.resize(low_res_image, bigger_size, cv2.INTER_LINEAR)\n",
    "\n",
    "        low_res_image = cv2.cvtColor(low_res_image, cv2.COLOR_BGR2YCR_CB)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2YCR_CB)\n",
    "\n",
    "        if type == 0:\n",
    "            low_res_image = cv2.copyMakeBorder(low_res_image, 0, 33, 0, 33, cv2.BORDER_CONSTANT, None, value=0)\n",
    "            image = cv2.copyMakeBorder(image, 0, 33, 0, 33, cv2.BORDER_CONSTANT, None, value=0)\n",
    "\n",
    "            for i in range(0, smaller_size[0], WINDOW_OFFSET):\n",
    "                for j in range(0, smaller_size[1], WINDOW_OFFSET):\n",
    "                    low_res.append(low_res_image[j:j+WINDOW_SIZE, i:i+WINDOW_SIZE, 0:1])\n",
    "\n",
    "            for i in range(0, bigger_size[0], WINDOW_OFFSET):\n",
    "                for j in range(0, bigger_size[1], WINDOW_OFFSET):\n",
    "                    originals.append(image[j:j+WINDOW_SIZE, i:i+WINDOW_SIZE, 0:1])\n",
    "        else:\n",
    "            low_res.append(low_res_image[:, :, 0:1])\n",
    "            originals.append(image[:, :, 0:1])\n",
    "\n",
    "    display_dataset_info(low_res, \"LOW RESOLUTION IMAGES, FACTOR: \" + str(factor), 5, 1, 5)\n",
    "    display_dataset_info(originals, \"ORIGINAL IMAGES, FACTOR: \" + str(factor), 5, 1, 5)\n",
    "\n",
    "    return low_res, originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(originals, low_res, epochs, factor):\n",
    "    model = Mutliresolution()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    for i in range(len(low_res)):\n",
    "        low_res[i] = expand(low_res[i])\n",
    "\n",
    "    for i in range(len(originals)):\n",
    "        originals[i] = expand(originals[i])\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataset = ImageDataset(low_res, originals)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch:' + str(epoch + 1) + ' of ' + str(epochs))\n",
    "\n",
    "        for step, (low_res, originals) in enumerate(dataloader):\n",
    "            if (step + 1) % 100 == 0:\n",
    "                print('training image:' + str(step + 1) + ' of ' + str(len(dataloader)))\n",
    "\n",
    "                low_res = low_res.to(device)\n",
    "                originals = originals.to(device)\n",
    "\n",
    "                pred = model(low_res)\n",
    "\n",
    "                loss = loss_fn(pred, originals)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "    torch.save(model, \"multiresolution-factor\" + str(factor) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(originals, low_res, unmodified, factor, model):\n",
    "    for step in range(len(low_res)):\n",
    "        original_image = torch.from_numpy(expand(originals[step])).to(device)\n",
    "        low_res_image = torch.from_numpy(expand(low_res[step])).to(device)\n",
    "        \n",
    "        smaller_size = (int(unmodified[step].shape[1] / factor), int(unmodified[step].shape[0] / factor))        \n",
    "        bigger_size = (int(unmodified[step].shape[1]), int(unmodified[step].shape[0]))\n",
    "        low_res_rgb = cv2.resize(unmodified[step], smaller_size, cv2.INTER_LINEAR)\n",
    "        low_res_rgb = cv2.resize(unmodified[step], bigger_size, cv2.INTER_LINEAR)\n",
    "\n",
    "        low_res_ycbcr = cv2.cvtColor(low_res_rgb, cv2.COLOR_BGR2YCR_CB)\n",
    "        low_res_ycbcr = low_res_ycbcr.transpose(-1, 0, 1)\n",
    "        low_res_ycbcr = np.expand_dims(low_res_ycbcr, axis = 0)\n",
    "        low_res_ycbcr = torch.from_numpy(low_res_ycbcr).to(device)\n",
    "\n",
    "        pred = model(low_res_image)\n",
    "        pred = pred.clamp(0, 1)\n",
    "\n",
    "        high_res_image = torch.concat([pred.unsqueeze(0), low_res_ycbcr[:, 1:3]], axis=1)\n",
    "\n",
    "        low_res_image_numpy = low_res_image[:, 0:1].squeeze().cpu().detach().numpy()\n",
    "        original_image_numpy = original_image[:, 0:1].squeeze().cpu().detach().numpy()\n",
    "        high_res_image_numpy = pred[:, 0:1].squeeze().cpu().detach().numpy()\n",
    "\n",
    "        if step < 3:\n",
    "            metrics = {\n",
    "                \"psnr_org_lr\": skimage.metrics.peak_signal_noise_ratio(original_image_numpy, low_res_image_numpy),\n",
    "                \"psnr_org_hr\": skimage.metrics.peak_signal_noise_ratio(original_image_numpy, high_res_image_numpy),\n",
    "                \"ssim_org_lr\": skimage.metrics.structural_similarity(original_image_numpy, low_res_image_numpy),\n",
    "                \"ssim_org_hr\": skimage.metrics.structural_similarity(original_image_numpy, high_res_image_numpy)\n",
    "            }\n",
    "            display_org_lr_hr(unmodified[step], low_res_rgb, high_res_image.cpu().detach().numpy()[0], metrics, (200, 160))\n",
    "\n",
    "        # display_image(pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(TRAIN_FOLDER)\n",
    "\n",
    "display_dataset_info(images, \"DATASET\", 15, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in range(2, 4):\n",
    "    low_res, originals = preprocess(images, factor, 0)\n",
    "    train(originals, low_res, EPOCHS, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in range(2, 4):\n",
    "    print(\"==========================================================================================\")\n",
    "    print(\"FACTOR: \" + str(factor))\n",
    "    print(\"==========================================================================================\")\n",
    "\n",
    "    model = torch.load(\"multiresolution-factor\" + str(factor) + \".pt\", map_location=device)\n",
    "\n",
    "    print(torchinfo.summary(model, (1, 1, 33, 33)))\n",
    "\n",
    "    display_filters(model.first_conv2d.weight.cpu().detach(), \"FACTOR \" + str(factor) + \" FIRST CONV2D LAYER FILTERS\")\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in range(2, 4):\n",
    "    model = torch.load(\"multiresolution-factor\" + str(factor) + \".pt\", map_location=device)\n",
    "\n",
    "    for dataset in [f.path for f in os.scandir(TEST_FOLDER) if f.is_dir()]:\n",
    "        images = load_images_from_folder(dataset + \"/test/\")\n",
    "\n",
    "        display_dataset_info(images, \"DATASET: \" + dataset.split(\"/\")[1] + \", FACTOR: \" + str(factor), 5, 1, 5)\n",
    "\n",
    "        low_res, originals = preprocess(images, factor, 1)\n",
    "\n",
    "        test(originals, low_res, images, factor, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6532684ccaeb1bcbbe852b7f75c67e6f1d55df7d386020fd37670376cbe3d2c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
